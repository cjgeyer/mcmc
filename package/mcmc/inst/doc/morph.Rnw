
\documentclass{article}

\usepackage{natbib}
\usepackage{graphics}
\usepackage{amsmath, amssymb}
\usepackage{indentfirst}
\usepackage[utf8]{inputenc}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}

% \VignetteIndexEntry{MCMC Morph Example}

\begin{document}

<<foo,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
foo <- packageDescription("mcmc")
@

\title{Morphemetric MCMC Example (mcmc Version \Sexpr{foo$Version})}
% $ (Just to make emacs syntax highlighting work properly)
\author{Leif T. Johnson \\ and Charles J. Geyer}
\maketitle

\section{Overview}

This is an example how to use morphemetric Markov chains as implemented in
the \verb@mcmc@ package in R.

Let $X$ be a $\mathbb{R}^k$ valued random variable with probability density
function (pdf), $f_X$.  Let $g$ be a diffeomorphism, and $Y=g(X)$.  Then the 
pdf of $Y$, $f_Y$ is given by the equation
\begin{equation}\label{eq:def-fy}
  f_Y(y) = f_X\bigl(g^{-1}(y)\bigr) | \nabla g^{-1}(y) |.
\end{equation}
Since $g$ is a diffeomorphism, we can draw inference about $X$ from information 
about $Y$ (and vice versa).  

It is not unusual for $f_X$ to either be known only up to a normalizing 
constant, or to be analytically intractable in other ways --- such as a large 
number of dimensions.  A common solution to this problem is to use Markov chain
Monte Carlo (MCMC) methods to learn about $f_X$ when this happens.

When using MCMC, a primary concern of the practicioner should be the question 
"Does the Markov chain converge fast enough to be useful?"  One very useful 
convergence rate is called \emph{geometrically ergodic} \textbf{CITE SOMETHING 
HERE}.

The \texttt{mcmc} package implements the Metropolis random-walk algorithm for 
arbitrary log unnormalized probability densities.  But the Metropolis 
random-walk algorithm does not always perform well.  As is demonstrated in 
\citet{johnson-geyer}, for one-to-one densities, $f_X$ and $f_Y$ defined as in
\eqref{eq:def-fy} a Metropolis random-walk $f_Y$ can be geometrically ergodic 
even though a Metropolis random-walk for $f_X$ is not.  Since the densities are
one-to-one, inference about $f_X$ can be drawn from the Markov chain for $f_Y$.

The \texttt{morph.metrop} and \texttt{morph} functions in the \texttt{mcmc} 
package provide this functionality, and this vignette gives a demonstration
on how to use them.

\section{$T$ Distribution}

\citet{johnson-geyer}[Example 4.2] show that a T distribution is
sub-exponentially light, and hence a Metropolis random-walk for a $T$
density cannot be geometrically ergodic, but, using the transformations
described in their Corollaries 1 and 2 will induce a target density,
$\pi_\gamma$ for which a Metropolis random-walk will be geometrically
ergodic.  In fact, we know from \citet{mengersen-tweedie} that for a
univariate target density, being exponentially light is a sufficient
condition for the geometric ergodicity of a Metropolis random-walk.  Hence
using the transformation described as $h_2$ in
\citet{johnson-geyer}[Corollary 2] will induce a target density for which a
Metroplis random-walk will be geometrically ergodic.  This section
demonstrates how to do this using the \texttt{mcmc} package for
\texttt{R}.  We will use a univariate $T$ distribution with 3 degrees of
freedom for a target density.

Passing a positive value for \texttt{b} to \texttt{morph} function will
create the aforementioned transformation, $h_2$.  It's as simple as 
<<>>=
library(mcmc)
h2 <- morph(b=1)
@ 
We can now see the induced density.  Note that \texttt{morph} works for
log unnormalized densities, so we need exponentiate the induced density to
plot it on the usual scale.
<<>>=
lud <- function(x) dt(x, df=3, log=TRUE)
lud.induced <- h2$lud(lud)
@ 
We can plot the two densities,
<<fig=TRUE>>=
x <- seq(-3, 3, length.out=400)
plot(x, exp(sapply(x, lud.induced)), lty=2, type="l",
     xlab="t", ylab="")
lines(x, exp(lud(x)), lty=1)
legend("topright", c("t-density", "induced density"), lty=1:2)
@ 
The \texttt{sapply} in this example is necessary, \texttt{lud.induced} is not 
vectorized.  Instead, it treats any vector passed as a single input, which
is rescaled (using an isomorphic transformation) and passed to
\texttt{lud}.  Compare the behavior of \texttt{lud} and
\texttt{lud.induced} in the following example.
<<>>=
lud(rep(0, 4))
lud(1)
lud.induced(rep(0, 4))
lud.induced(1)
@
Because \texttt{dt} is vectorized, \texttt{lud} is vectorized so it maps 
vectors to vectors, but \texttt{lud.induced} treats the vector as a single
input.

\section{Multinomial Distribution with a Conjugate Prior}

\begin{thebibliography}{}

\bibitem[Gelman et al.(1996)Gelman, Roberts, and Gilks]{grg}
Gelman, A., G.~O. Roberts, and W.~R. Gilks (1996).
\newblock Efficient Metropolis jumping rules.
\newblock In \emph{Bayesian Statistics, 5 (Alicante, 1994)}, pp.~599--607.
  Oxford University Press.

\bibitem[Geyer(1992)]{practical}
Geyer, C.~J. (1992).
\newblock Practical Markov chain Monte Carlo (with discussion).
\newblock \emph{Statistical Science}, 7, 473--511.

\bibitem[Geyer and Thompson(1995)]{geyer-temp}
Geyer, C.~J. and E.~A. Thompson (1995).
\newblock Annealing Markov chain Monte Carlo with applications to
    ancestral inference.
\newblock \emph{Journal of the American Statistical Association}, 90, 909--920.

\bibitem[Johnson and Geyer(submitted)]{johnson-geyer}
Johnson, L.~T. and C.~J. Geyer (submitted).
\newblock Variable Transformation to Obtain Geometric Ergodicity
    in the Random-walk Metropolis Algorithm.
\newblock \emph{Annals of Statistics}.

\bibitem[Mengersen and Tweedie(1996)]{mengersen-tweedie}
  Mengersen, K.L. ad R. L. Tweedie (1996).
\newblock Rates of convergence of the {H}astings and {M}etropolis algorithms.
\newblock \emph{Annals of Statistics}, 24, 101--121.

\end{thebibliography}

\end{document}
