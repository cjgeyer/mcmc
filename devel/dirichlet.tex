
\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{indentfirst}
\usepackage{natbib}
\usepackage{url}

\DeclareMathOperator{\aff}{aff}

\newcommand{\real}{\mathbb{R}}

\newcommand{\set}[1]{\{\, #1 \,\}}

\begin{document}

\title{Sampling Constrained Dirichlet Distributions}

\author{Charles J. Geyer}

\maketitle

\section{Introduction} \label{sec:intro}

Let $C$ denote the convex polytope that consists of all points $x$ in
$d$-dimensional Euclidean space satisfying the constraints
\begin{enumerate}
\item[(i)] $x \ge 0$,
\item[(ii)] $\sum_{i = 1}^d x_i = 1$,
\item[(iii)] $A_1 x \le b_1$, and
\item[(iv)] $A_2 x = b_2$,
\end{enumerate}
where the inequalities work componentwise, as in R \citep{r},
where $A_1$ is a matrix
and $b_1$ is a vector having dimensions such that (iii) makes sense and
similarly for $A_2$ and $b_2$, and where either or both of (iii) and (iv)
may be absent.  Let $\alpha$ be a vector having positive-real-valued
components.  We call the probability distribution having unnormalized density
\begin{equation} \label{eq:unnormalized-density}
   h(x) = \prod_{i = 1}^d x_i^{\alpha_i - 1}
\end{equation}
with respect to Lebesgue measure on $C$ the \emph{constrained Dirichlet
distribution} on $C$ having parameter vector $\alpha$.
By Lebesgue measure on $C$ we mean Lebesgue measure on the affine hull
of $C$ restricted to $C$ (more on this below).

We discuss efficient Markov chain sampling of such distributions.
The R package \texttt{polyapost} \citep{polyapost} already does this
for the special case when the Dirichlet distribution is uniform
($\alpha_i = 1$ for all $i$), and we will use some
of their ideas, in particular, the hit-and-run sampler
\citep*{smith,hit-run-one,hit-run-two}, but we will also use some other ideas.
The R package \texttt{hitandrun} \citep{hitandrun}
also does the uniform distribution on a
convex polytope using a hit-and-run sampler (competing with \texttt{polyapost}).

There do not appear to be any R packages on CRAN that do constrained Dirichlet
distribution sampling, other than general purpose MCMC packages like
the R package \texttt{mcmc} \citep{mcmc}, which uses a Metropolis
random-walk sampler (\citealp{metropolis-et-al};
\citealp[Section~2.3.2]{tierney})
and simulates any distribution specified by a user
supplied function.  We do not use this because we want to make a sampler
specifically tuned to the constrained Dirichlet problem.

We will use the R package \texttt{rcdd} \citep*{rcdd} to handle
the computational geometry of the constraint set $C$.

Define the vector $\lambda$ having components
\begin{equation} \label{eq:lambda}
   \lambda_i = \frac{\alpha_i}{\alpha_1 + \cdots + \alpha_d}
\end{equation}
and let $\Lambda$ denote the diagonal matrix having the vector $\lambda$
as its diagonal.  Then the normal approximation to the constrained
Dirichlet distribution is
the $\text{Normal}(\lambda, \Lambda)$ distribution
restricted to $C$ \citep[Theorem~4.2]{geyer-meeden}.  For $\alpha$ having
all components ``large'' this will be a very good approximation, so we want
to use this to speed up the simulation.  Otherwise, this may be a bad
approximation, so we want to use the approximation in a way that still
allows simulation of the exact distribution of interest.

``Independence'' updates (\citealp{hastings};
\citealp[Section~2.3.3]{tierney}) do this.
In detail, propose $y$ from this constrained normal distribution,
then do Metropolis-Hastings rejection: calculate
$$
    R = \frac{f(y) \varphi(x)}{f(x) \varphi(y)}
$$
where $\varphi$ is the density of the normal proposal and accept the
proposal with probability $\min(1, R)$.
We combine hit-and-run and independence updates to make a hybrid sampler
\citep[Section~2.4]{tierney} better than either update used by itself.

\section{Details}

\subsection{Computational Geometry}

A convex polytope like our constraint set $C$ has two different
representations, as the set satisfying a finite set of linear equalities
and inequalities, like those in the list in Section~\ref{sec:intro},
and as the convex hull
of the vertices of the set.   The R package \texttt{rcdd} calls these
the H-representation and the V-representation
(H for half space and V for vertex).  The function
\texttt{scdd} in that package goes between them.

Modify the definition of $A_1$, $b_1$, $A_2$, and $b_2$ in the list in
Section~\ref{sec:intro}
so that $A_1$ and $b_1$ include the nonnegativity constraints (i) and
$A_2$ and $b_2$ include the sum to one constraint (ii).  Then the
R function \texttt{makeH} in the \texttt{rcdd} package makes
an H-representation for $C$.

\subsubsection{Vertices}

Apply the \texttt{scdd} function to the H-representation.
It produces the V-representation which gives the set of vertices of $C$.
Call this vertex set $V$.

If $V$ is empty or a singleton set, we return an error to the user.
If $V$ is empty, then $C$ is empty too, so the user has indeed made an
error in specifying inconsistent constraints.  There is no probability
distribution on the empty set.
If $V$ is a singleton set, then $C = V$, so while, strictly speaking,
the user has not made an error, the probability distribution is trivial.
There is only one probability distribution on a one-point sample space,
and we do not need MCMC to describe it.

Thus in what follows we assume $C$ has at least two vertices.

Let $x_V$ denote any convex combination of the points in $V$.
This is a point known to be in $C$, which we can use as the starting
point for our MCMC sampler.

\subsubsection{Affine Hull}

The first issue we have to deal with is that $C$ is not $d$-dimensional.
What dimension is it?  We can't tell from our H-representation, which may
have hidden redundancies.  The R function \texttt{redundant}
in the \texttt{rcdd} package produces an equivalent H-representation that
is non-redundant (the equality constraints are linearly independent,
and the inequality constraints are true inequalities rather than equalities,
that is, they actually hold with strict inequality for some points in $C$).

For our purposes here we only need the equality constraints from
this non-redundant H-representation.  Extract its rows
representing equality constraints, giving yet another H-representation,
which represents the affine hull of $C$, denoted $\aff C$.

Apply the \texttt{scdd} function to the H-representation of $\aff C$.
It produces the V-representation of $\aff C$, which (unlike the
V-representation of a polytope) contains both points and directions.
In this case there will be exactly one point and one or more directions
(this follows from our assumption that $C$ has at least two vertices,
which implies that the dimension of $\aff C$ is at least one and from
our assumption that $C$ is contained in the unit simplex so it does
not contain the origin, $\aff C$ is not a vector space but rather a
translate of a vector space, so there must be one point in the
V-representation, which is the translation).

Let $M$ denote the matrix whose columns are the directions in the
V-representation of $\aff C$ so the column space of $M$ is the vector
subspace of $\real^d$ that is parallel to $\aff C$.  Let $p$ be the
column dimension of $M$ (the number of directions in the V-representation
of $\aff C$).  Then
\begin{equation} \label{eq:map}
   w \mapsto x_V + M w
\end{equation}
is an isomorphism between $\real^p$ and $\aff C$ (it is one-to-one and onto).
We think of \eqref{eq:map} as providing a coordinatization of $\aff C$.

In order to finish our description of $C$ we need to describe the inequality
constraints in our new coordinates.  The constraints $A_1 x \le b_1$ in
the original coordinates are $A_1 x_V + A_1 M w \le b_1$
in the new coordinates, and we denote this $A_3 w \le b_3$ where
\begin{align*}
   A_3 & = A_1 M
   \\
   b_3 & = b_1 - A_1 x_V
\end{align*}
Define
$$
   C_3 = \set{ w \in \real^p : A_3 w \le b_3 }.
$$
This is our constraint set $C$ expressed in our new coordinates
(note that there are now no equality constraints).
We are using the convention here, mentioned at the beginning of this section,
that $A_1$ and $b_1$ now determine all inequality constraints, items (i)
and (iii) on the list in Section~\ref{sec:intro}, rather than just (iii).

\subsection{The Distribution}

\subsubsection{Exact}

We now need to map the unnormalized density \eqref{eq:unnormalized-density}
to the new coordinates.  Define $h_3$ by
\begin{equation} \label{eq:h-new}
   h_3(w) = h(x_V + M w), \qquad w \in C_3.
\end{equation}
Our problem is now to sample the distribution concentrated on $C_3$
having unnormalized density $h_3$ with respect to Lebesgue measure.
The map \eqref{eq:map} maps our samples of this distribution back to
the original coordinates (which are the only coordinates the user knows
about).

The map \eqref{eq:map} maps Lebesgue measure on $\real^p$ to Lebesgue measure
on $\aff C$.  Different choices of $M$ (only the column space of $M$ matters,
not its actual components) give different notions of Lebesgue measure on
$\aff C$, but since the Jacobian of a linear transformation is constant,
these different notions of Lebesgue measure are scalar multiples of each other.
Since we are using unnormalized densities, it does not matter which choice
of Lebesgue measure on $\aff C$ (which choice of $M$) is used.
We always get the same distribution after normalization.

\subsubsection{Approximate}

The normal approximation to the Dirichlet distribution with
parameter vector $\alpha$ has unnormalized density
$$
   h_{\text{approx}}(x)
   =
   \exp\left(- \tfrac{1}{2} (x - \lambda)^T \Lambda^{- 1} (x - \lambda) \right)
$$
where $\lambda$ is given by \eqref{eq:lambda} and $\Lambda$ is diagonal
with $\lambda$ on the diagonal \citep[Theorem~4.2]{geyer-meeden}.

We need to move this to the new coordinates.  Define
\begin{equation} \label{eq:h-approx-new}
   h_4(w)
   =
   \exp\left(- \tfrac{1}{2} (x_V + M w - \lambda)^T
   \Lambda^{- 1} (x_V + M w - \lambda) \right).
\end{equation}
The definition of $h_4$ given above is perfectly good for running
Metropolis-Hastings updates to sample it.  But we also want to
be able to generate independent samples from this normal distribution.
And for that we need to know its mean vector and variance matrix.
Let them be denoted $\mu$ and $\Sigma$.  Then we have
$$
   (x_V + M w - \lambda)^T \Lambda^{- 1} (x_V + M w - \lambda)
   =
   (w - \mu)^T \Sigma^{- 1} (w - \mu) + \text{constant}
$$
from which we see that
\begin{equation} \label{eq:h-approx-new-variance}
   \Sigma^{- 1} = M \Lambda^{- 1} M
\end{equation}
and
$$
   M^T \Lambda^{- 1} (\lambda - x_V) = \Sigma^{- 1} \mu
$$
so
\begin{equation} \label{eq:h-approx-new-mean}
   \mu = \left( M \Lambda^{- 1} M \right)^{- 1}
   M^T \Lambda^{- 1} (\lambda - x_V)
\end{equation}
\citep[compare equation (25) in][]{geyer-meeden}.

\section{Sampler}

\subsection{Hit-and-Run Updates}

The basic idea of a hit-and-run update is to make a move along a randomly
chosen direction.    If $w \in C_3$ is the current position of the Markov
chain, choose a random direction $d \in \real^p$ and then propose to move
to $z = w + s d$, where $s$ is any real number such that $z \in C_3$.
The proposal is then Metropolis rejected so update preserves the desired
stationary distribution.

Hit-and-run samplers differ in how the random direction is chosen and in
how the proposal is made given the chosen direction.  So we have two
different issues to consider.

\subsubsection{Choosing the Direction}

The conventional way to choose a random direction is to generate
a random vector from a spherically symmetric normal distribution.
The R packages \texttt{polyapost} and \texttt{hitandrun} do this.
This choice is very problematic, however because it has no relation
to the problem at hand.  We see this in the map \eqref{eq:map} which
is arbitrary (recall that only the column space of $M$ matters) but
which affects how the unit ball in $\real^p$ maps into $\aff C$,
which is what really matters.  (Spherical symmetry in $\aff C$ is not
even well defined, but however one might define it, the definition
should not be arbitrary, and the map \eqref{eq:map} is arbitrary,
in the sense that if $M$ is changed but its column space is not changed,
then the map works just as well.)
Moreover, this method pays no attention
to the inequality constraints (specified by $A_3$ and $b_3$) nor any
attention to the unnormalized density $h_3$.

There is no reason to follow this convention.  Any method of choosing
a random direction will do.  \citet{smith} and \citet{hit-run-one}
discuss choosing the random direction to be along one of the coordinate axes,
so the directions are chosen from a discrete set, not a continuum.
But even more generality is possible.
\citet[Section~1.12.8]{geyer-intro} points out
that any state-independent mixture of updates that preserve the desired
stationary distribution also preserves that distribution.  In this context
that means any distribution on directions whatsoever works, so long as the
distribution on directions is fixed (it does not change from iteration
to iteration and does not depend on the state of the Markov chain).

We want a proposal that pays attention to the problem at hand, in particular,
to the convex polytope $C_3$ that is the constraint set in our new coordinates.
Let $V_3$ denote the set of vertices of $C_3$.  These can be found either
by mapping the points of $V$ (vertices in old coordinates) through the
inverse mapping of \eqref{eq:map} or by running the R function \texttt{scdd}
on the H-representation given by $A_3$ and $b_3$.  Every ordered pair of
distinct points of $V_3$ determines a direction (from the first point to the
second point).  We propose to choose hit-and-run directions from this
(finite) set of directions.  There is no need to choose these directions
with equal probability; it seems more reasonable to choose so that directions
between vertices that are farther apart are chosen with higher probability
(the idea being that the main thing that makes a hit-and-run sampler mix
slowly is difficulty finding directions in the polytope in which long moves
can be made).  Exactly what nonuniform distribution should be used is an
open question for which we have no good argument.  For now we propose
(without having given the subject much thought) to have the probability
of a direction proportional to the distance between the vertices that
specify the direction.

Since we are now proposing to sample a fairly complicated discrete
distribution, how do we do that?  An efficient method is Walker's method
of aliases \citep{walker}.  Rather than bother to implement it ourselves,
we propose to steal the code
in the C function \verb@walker_ProbSampleReplace@ in the file
\texttt{src/main/random.c} in the current R distribution \citep{r},
which is 3.1.0 at the time of this writing.  This is the function R
itself uses to implement the R function \texttt{sample} when sampling
with replacement is requested and the number of samples requested
is greater than 200.  We would have to break up the function
\verb@walker_ProbSampleReplace@ in R into two functions, which is
easy to do, since the two parts are commented.  The first part creates
the alias tables, the second part generates samples, and both parts
are commented as such.  We would generate the alias tables exactly once,
and generate one ``sample'' for each hit-and-run update we do.
There is no problem in our stealing this code because R is free software
(GPL version 2 or any later version) so we only need use a GPL-compatible
licence for our software, which is required to be a CRAN package anyway.

\subsubsection{Proposal given the Direction}

Given the direction, we need to propose a point on the line
$$
   L = \set{ x + s d : s \in \real }
$$
where $x$ is the current point and $d$ is the chosen direction.  Again
there is a standard proposal, and it is uniform on the part of the line
$L \cap C_3$ that is in the constraint set.

This uniform proposal has the virtue of being a Metropolis proposal,
so one uses the odds ratio
\begin{equation} \label{eq:metropolis-ratio}
   R = \frac{h_3(y)}{h_3(x)}
\end{equation}
where $x$ is the current state of the Markov chain and $y$ is the proposal
uniformly distributed on $L \cap C_3$, to do Metropolis rejection: accept
the proposal with probability $\min(1, R)$.

\subsection{Independence Updates}

All of the work in setting this up has already been described.  Simulate
a multivariate normal random vector $y$ in $\real^p$ having mean vector $\mu$
and variance matrix $\Sigma$ given by \eqref{eq:h-approx-new-mean}
and \eqref{eq:h-approx-new-variance}.  Calculate the Hastings ratio
$$
    R = \frac{h_3(y) h_4(x)}{h_3(x) h_4(y)}
$$
where $x$ is the current state and $h_3$ and $h_4$ are as described above
\eqref{eq:h-new} and \eqref{eq:h-approx-new}.  Accept this proposal
with probability $\min(1, R)$.

Note that all of these proposals are from exactly the same normal distribution.
Hence we need not do the set-up more than once.  One Cholesky decomposition
of $\Sigma$ suffices.  If $\Gamma \Gamma^T = \Sigma$,
then $z = \mu + \Gamma u$, has the desired normal distribution when $u$
is standard multivariate normal.

\subsection{Near Vertex Updates}

We are concerned that none of the updates described above will work well
when all of the components of the parameter vector $\alpha$ of the Dirichlet
distribution are near zero, in which case the distribution will be concentrated
near some of the vertices of $C_3$ (recall that we are denoting this set
of vertices $V_3$).  In particular, it is concentrated near vertices having
some components equal to zero when expressed in the original coordinates,
but we won't bother to distinguish these vertices.

We are going to do something rather tricky that is not a special case
of the Metropolis-Hastings algorithm, although it is a special case
of what \citet{geyer-intro} calls the Metropolis-Hastings-Green algorithm.
In particular, it uses what \citet[Section~1.17.1]{geyer-intro} calls
state-dependent mixing.

Fix $\varepsilon > 0$ less than half minimum distance between any pair of
elements of $V_3$.  We only do the update described here when the
current state $x$ of the Markov chain has distance less than $\varepsilon$
from some element of $V_3$.  Then we propose $y$ uniformly
distributed on the $\varepsilon$-balls centered at the elements of $V_3$
except for the $\varepsilon$-ball containing the current state $x$.
This is a Metropolis proposal. Hence the proposal is accepted with
probability $\min(1, R)$ where $R$ is given by \eqref{eq:metropolis-ratio}.

The fact that it only kicks in when the current state is within $\varepsilon$
of some vertex does not matter; this is taken care of by the state-dependent
mixing argument.

\begin{thebibliography}{}

\bibitem[B\'elisle et~al.(1993)B\'{e}lisle, Romeijn and Smith]{hit-run-one}
B\'{e}lisle, C. J.~P., Romeijn, H.~E., and Smith, R.~L. (1993).
\newblock Hit-and-run algorithms for generating multivariate distributions.
\newblock \emph{Mathematics of Operations Research}, \textbf{18}, 255--266.

\bibitem[Chen and Schmeiser(1993)]{hit-run-two}
Chen, M.-H. and Schmeiser, B. (1993).
\newblock Performance of the Gibbs, hit-and-run, and Metropolis samplers.
\newblock \emph{Journal of Computational and Graphical Statistics},
   \textbf{2}, 251--272.

\bibitem[Geyer(2011)]{geyer-intro}
Geyer, C. J. (2011).
\newblock Introduction to Markov chain Monte Carlo.
\newblock In \emph{Handbook of Markov Chain Monte Carlo}, edited by
    Brooks, S., Gelman, A., Jones, G., and Meng, X.-L., pp.~3--48.
\newblock Boca Raton, FL: Chapman \& Hall/CRC.

\bibitem[Geyer and Johnson(2014)]{mcmc}
Geyer, C.~J. and Johnson, L.~T. (2014).
\newblock R package \texttt{mcmc}: Markov chain Monte Carlo,
    version 0.9-3.
\newblock \url{http://cran.r-project.org/package=mcmc}.

\bibitem[Geyer and Meeden(2013)]{geyer-meeden}
Geyer, C.~J. and Meeden, G.~D. (2013).
\newblock Asymptotics for Constrained Dirichlet Distributions.
\newblock \emph{Bayesian Analysis}, \textbf{8}, 89--110.

\bibitem[Geyer, et al.(2014)Geyer, Meeden and Fukuda]{rcdd}
Geyer, C.~J., Meeden, G.~D. and Fukuda, K. (2014).
\newblock R package \texttt{rcdd}: Computational geometry,
    version 1.1-8.
\newblock \url{http://cran.r-project.org/package=rcdd}

\bibitem[Hastings(1970)]{hastings}
Hastings, W.~K. (1970).
\newblock Monte Carlo sampling methods using Markov chains and their
    applications.
\newblock \emph{Biometrika}, \textbf{57}, 97--109.

\bibitem[{Lazar et~al.(2008)Lazar, Meeden, and Nelson}]{lazar-meeden-nelson}
Lazar, R., Meeden, G., and Nelson, D. (2008).
\newblock A noninformative {B}ayesian approach to finite population sampling
  using auxiliary variables.
\newblock \emph{Survey Methodology}, \textbf{34}, 51--64.

\bibitem[{Meeden and Lazar(2014)}]{polyapost}
Meeden, G. and Lazar, R. (2014).
\newblock R package \texttt{polyapost}: Simulating from the Polya posterior,
    version 1.1-6.
\newblock \url{http://cran.r-project.org/package=polyapost}.

\bibitem[Metropolis, et al.(1953)Metropolis, Rosenbluth, Rosenbluth, Teller
    and Teller]{metropolis-et-al}
Metropolis, N., Rosenbluth, A.~W., Rosenbluth, M.~N., Teller, A.~H., and
    Teller, E. (1953).
\newblock Equation of state calculations by fast computing machines.
\newblock \emph{Journal of Chemical Physics}, \textbf{21}, 1087--1092.

\bibitem[R Core Team(2014)]{r}
R Core Team (2014).
\newblock R: A language and environment for statistical computing.
\newblock Vienna: R Foundation for Statistical Computing.
\newblock \url{http://www.R-project.org/}.

\bibitem[Smith(1984)]{smith}
Smith, R.~L. (1984).
\newblock "Efficient Monte Carlo procedures for generating points
    uniformly distributed over bounded regions.
\newblock \emph{Operations Research}, \textbf{32}, 1296--1308.

\bibitem[Tierney(1994)]{tierney}
Tierney, L. (1994).
\newblock Markov chains for exploring posterior distributions (with discussion).
\newblock \emph{Annals of Statistics}, \textbf{22}, 1701--1762.

\bibitem[van Valkenhoef(2014)]{hitandrun}
van Valkenhoef, G. (2014).
\newblock R package \texttt{hitandrun}: ``Hit and run'' method for sampling
    uniformly from convex shapes, version 0.4-1.
\newblock \url{http://cran.r-project.org/package=hitandrun}.

\bibitem[Walker(1977)]{walker}
Walker, A.~J. (1977).
\newblock An efficient method for generating discrete random variables
    with general distributions.
\newblock \emph{ACM Transactions on Mathematical Software}, \textbf{3},
    253--256.

\end{thebibliography}

\end{document}

