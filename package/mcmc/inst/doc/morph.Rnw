\documentclass{article}

\usepackage{natbib}
\usepackage{graphics}
\usepackage{amsmath, amssymb}
\usepackage{indentfirst}
\usepackage[utf8]{inputenc}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\E}{E}
\newcommand{\inner}[1]{\langle #1 \rangle}

% \VignetteIndexEntry{MCMC Morph Example}

\begin{document}

<<foo,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
foo <- packageDescription("mcmc")
@

\title{Morphemetric MCMC Example (mcmc Version \Sexpr{foo$Version})}
% $ (Just to make emacs syntax highlighting work properly)
\author{Leif T. Johnson \\ and Charles J. Geyer}
\maketitle

\section{Overview}

This is an example how to use morphemetric Markov chains as implemented in
the \verb@mcmc@ package in R.

Let $X$ be a $\mathbb{R}^k$ valued random variable with probability density
function (pdf), $f_X$.  Let $g$ be a diffeomorphism, and $Y=g(X)$.  Then the 
pdf of $Y$, $f_Y$ is given by the equation
\begin{equation}\label{eq:def-fy}
  f_Y(y) = f_X\bigl(g^{-1}(y)\bigr) | \nabla g^{-1}(y) |.
\end{equation}
Since $g$ is a diffeomorphism, we can draw inference about $X$ from information 
about $Y$ (and vice versa).  

It is not unusual for $f_X$ to either be known only up to a normalizing 
constant, or to be analytically intractable in other ways --- such as a large 
number of dimensions.  A common solution to this problem is to use Markov chain
Monte Carlo (MCMC) methods to learn about $f_X$ when this happens.

When using MCMC, a primary concern of the practicioner should be the question 
"Does the Markov chain converge fast enough to be useful?"  One very useful 
convergence rate is called \emph{geometrically ergodic} \textbf{CITE SOMETHING 
HERE}.

The \texttt{mcmc} package implements the Metropolis random-walk algorithm for 
arbitrary log unnormalized probability densities.  But the Metropolis 
random-walk algorithm does not always perform well.  As is demonstrated in 
\citet{johnson-geyer}, for one-to-one densities, $f_X$ and $f_Y$ defined as in
\eqref{eq:def-fy} a Metropolis random-walk $f_Y$ can be geometrically ergodic 
even though a Metropolis random-walk for $f_X$ is not.  Since the densities are
one-to-one, inference about $f_X$ can be drawn from the Markov chain for $f_Y$.

The \texttt{morph.metrop} and \texttt{morph} functions in the \texttt{mcmc} 
package provide this functionality, and this vignette gives a demonstration
on how to use them.

\section{$T$ Distribution}

\citet{johnson-geyer}[Example 4.2] show that a T distribution is
sub-exponentially light, and hence a Metropolis random-walk for a $T$
density cannot be geometrically ergodic, but, using the transformations
described in their Corollaries 1 and 2 will induce a target density,
$\pi_\gamma$ for which a Metropolis random-walk will be geometrically
ergodic.  In fact, we know from \citet{mengersen-tweedie} that for a
univariate target density, being exponentially light is a sufficient
condition for the geometric ergodicity of a Metropolis random-walk.  Hence
using the transformation described as $h_2$ in
\citet{johnson-geyer}[Corollary 2] will induce a target density for which a
Metroplis random-walk will be geometrically ergodic.  This section
demonstrates how to do this using the \texttt{mcmc} package for
\texttt{R}.  We will use a univariate $T$ distribution with 3 degrees of
freedom for a target density.

Passing a positive value for \texttt{b} to \texttt{morph} function will
create the aforementioned transformation, $h_2$.  It's as simple as 
<<>>=
library(mcmc)
h2 <- morph(b=1)
@ 
We can now see the induced density.  Note that \texttt{morph} works for
log unnormalized densities, so we need exponentiate the induced density to
plot it on the usual scale.
<<>>=
lud <- function(x) dt(x, df=3, log=TRUE)
lud.induced <- h2$lud(lud)
@ 
We can plot the two densities,
<<fig=TRUE>>=
x <- seq(-3, 3, length.out=400)
plot(x, exp(sapply(x, lud.induced)), lty=2, type="l",
     xlab="t", ylab="")
lines(x, exp(lud(x)), lty=1)
legend("topright", c("t-density", "induced density"), lty=1:2)
@ 
The \texttt{sapply} in this example is necessary, \texttt{lud.induced} is not 
vectorized.  Instead, it treats any vector passed as a single input, which
is rescaled (using an isomorphic transformation) and passed to
\texttt{lud}.  Compare the behavior of \texttt{lud} and
\texttt{lud.induced} in the following example.
<<>>=
lud(rep(0, 4))
lud(1)
lud.induced(rep(0, 4))
lud.induced(1)
@
Because \texttt{dt} is vectorized, \texttt{lud} is vectorized so it maps 
vectors to vectors, but \texttt{lud.induced} treats the vector as a single
input, which is transformed and passed to \texttt{lud}.

Running a Markov chain for the induced density is done with
\texttt{morph.metrop}.
<<>>=
out <- morph.metrop(lud, 0, blen=100, nbatch=100, morph=morph(b=1))
@ 
The content of \texttt{out\$batch} is on the scale of used by
\texttt{lud}.  Once the transformation has been set, no adjustment is
needed (unless you want to change transformations).  We start by adjusting
the scale.
<<>>=
# adjust scale to find a roughly 20% acceptance rate
out$accept
@ 
An acceptance rate of \Sexpr{round(100 * out$accept, 1)}\%
%$ to fix emacs highlighting
is probably too high.  By increasing the scale of the proposal distribution
we can bring it down towards 20\%.
<<>>=
out <- morph.metrop(out, scale=4)
out$accept
@ 
We now use this Markov chain to estimate 
\begin{equation}\label{eq:e-abs-t}
  \E(|T|)
\end{equation}
for the $T$ distribution with 3 degrees of freedom.  We'll start with an
estimate from 1000 iterations of the chain.  
<<>>=
out <- morph.metrop(out, blen=1, nbatch=1e4, outfun=abs)
@ 
We're going to use over-lapping batch means to estimate \eqref{eq:e-abs-t},
but first we need to establish a reasonable batch length.
<<fig=TRUE>>=
acf(out$batch, lag.max=200)
@ 
It looks like a batch length of 100 will be sufficiently long.
<<>>=
mcse <- sqrt(olbm(out$batch, 100))
est.mean <- colMeans(out$batch)
est.mean
mcse
@ 
The variable \texttt{est.mean} gives our MCMC estimate of
\eqref{eq:e-abs-t} and \texttt{mcse} gives the Monte Carlo standard error
of our estimate.  Because the underlying Markov chain is geometrically
ergodic and $\E(|T|^2)$ is finite, there is a central limit theorem for our
estimate.  The confidence interval for \eqref{eq:e-abs-t} is given by
<<>>=
est.mean + mcse * 1.96 * c(-1,1)
@ 
If a narrower confidence interval is desired, the Markov chain can be run
longer.

Note that when calculating our estimate and the Monte Carlo standard error
we are not concerned with what was happening on the transformed scale.  The
\texttt{morph.metrop} function seamlessly does this for us.

\subsection{Simulation Study}

<<>>=
n.sims <- 300
n.iter <- 2000
@ 

To show the utility of the transformation, we will study the distribution
of estimates of $\E(|T|)$ for a univariate $T$ random variable with three
degrees of freedom.  We will consider two different estimation methods:
\begin{enumerate}
\item \label{enum:rw} We will estimate $\E(|T|)$ using a random-walk
  Metropolis algorithm for the $T$ density by using the \texttt{metrop}
  function.  \citet{jarner-roberts} demonstrate that a central limit
  theorem does not hold for these estimates.
\item \label{enum:rw-induced} We will estimate $\E(|T|)$ using a
  random-walk Metroplis algorithm for the density induced by setting
  \texttt{b=1} in the \texttt{morph.metrop} function.
  \citet{johnson-geyer} demonstrate that a central limit does hold for
  these estimates.
\end{enumerate}

To demonstrate this we will estimate $\E(|T|)$ \Sexpr{n.sims} times with
each method, in each case using a Markov chain \Sexpr{n.iter} iterations
long, and then examine the normality of the resulting estimates.

We have already determined that for the induced density, a scale of 4 is
about right.  Before doing the simulations we will establish a reasonable
scale to use for the random-walk Metropolis altorithm run for the $T$
density.

<<>>=
out <- metrop(lud, 0, blen=1000, nbatch=1)
out$accept
out <- metrop(out, scale=4)
out$accept
out <- metrop(out, scale=6)
out$accept
@ 
A scale of 6 appears to be about right.  It's now easy to run these
simulations.

<<runsims>>=
t.estimates <- 
  replicate(n.sims, 
            metrop(lud, 0, blen=n.iter, nbatch=1, scale=6, outfun=abs)$batch)
induced.estimates <- 
  replicate(n.sims, 
            morph.metrop(lud, 0, blen=n.iter, nbatch=1, scale=4, outfun=abs, 
                         nspac=1, morph=morph(b=1))$batch)
@ 

Now we can look at the distribution of our estimates.  Since this is a
``toy'' example, we can establish a truth value using Good Old Fashioned
Monte Carlo (GOFMC).

<<establishtruth>>=
n.truth <- 1000000
t.values <- abs(rt(n.truth, df=3))
truth <- mean(t.values)
truth.se <- 1.96 * sd(t.values) / sqrt(n.truth)
truth
truth.se
@
This estimates the true value out to $\pm \Sexpr{signif(truth.se, 1)}$.

Next we can look at histograms of the estimates.

<<fig=TRUE>>=
par(mfrow=c(1,2))
hist(t.estimates, xlab="", main="Estimates from R-W on T")
lines(rep(truth, 2), c(0, length(t.estimates)), col="red")
lines(rep(mean(t.estimates), 2), c(0, length(t.estimates)), lty=2)
hist(induced.estimates, xlab="",
     main="Estimates from R-W\non Induced Distribution")
lines(rep(truth, 2), c(0, length(induced.estimates)), col="red")
lines(rep(mean(induced.estimates), 2), c(0, length(induced.estimates)), lty=2)
@ 

Notice that the distribution coming from the estimates based on the
random-walk Metropolis algorithm for the induced
density are symmetric and follow a normal distribution, but the estimates
based on the random-walk Metropolis algorithm for the $T$ density do not
follow a symmetric distribution.  

We can also look at mean value of our estimates (which are themselves mean
values).  
<<t-abs-mean-estimates>>=
mean(t.estimates)
sd(t.estimates) / sqrt(length(t.estimates))
mean(t.estimates) + 
  sd(t.estimates) / sqrt(length(t.estimates)) * 1.96 * c(-1, 1)
mean(induced.estimates)
sd(induced.estimates) / sqrt(length(induced.estimates))
mean(induced.estimates) + 
  sd(induced.estimates) / sqrt(length(induced.estimates)) * 1.96 * c(-1, 1)
@ 

The following Q-Q plots for the standardized estimates support this theory.

<<fig=TRUE>>=
par(mfrow=c(1,2))
t.standardized <- (t.estimates - mean(t.estimates)) / sd(t.estimates)
induced.standardized <- 
  (induced.estimates - mean(induced.estimates)) / sd(induced.estimates)
qqnorm(t.standardized, main="Q-Q Plot\nEstimates from T")
qqline(t.standardized)
qqnorm(induced.standardized, main="Q-Q Plot\nEstimates from Induced Density")
qqline(induced.standardized)
@ 

Finally, we can examine the normality of the estimates with
Komogorov-Smirnov tests.

<<abs-t-kstest>>=
t.kstest <- ks.test(t.standardized, pnorm)
induced.kstest <- ks.test(induced.standardized, pnorm)
t.kstest$p.value
induced.kstest$p.value
@ 

Comparing the standardized estimates to a Normal$(0, 1)$ distribution gives
the p-values of \Sexpr{signif(t.kstest[["p.value"]], 3)} for the standardized 
estimates coming from the random-walk Metropolis algorithm for the
$t$-density, and \Sexpr{signif(induced.kstest[["p.value"]], 3)}
for the estimates coming from the random-walk Metropolis algorithm for the 
induced density.  Thus, there is strong evidence that the esimates deriving
from the un-transformed $t$-density do not follow a Normal distribution,
but there is \emph{no} evidence that the estimates deriving from the induced
density \emph{do not} follow a Normal distribution.  

\section{Binomial Distribution with a Conjugate Prior}

We demonstrate a morphemetric Markov chain using the \texttt{UCBAdmisions}
data set included in \texttt{R}, (use \texttt{help(UCBAdmissions)} to see
details of this data set).  We will model the probability of a student
being admitted or rejected, using the sex of the student and the department
that the student applied to as predictor variables.  For our prior, we
naively assume that 30\% of all students are admitted, independent of sex
or department.  As this is a naive prior, we will only add 5 students to
each gender-department combination.  This will not give the prior much
weight, most of the information in the posterior distribution will be from
the data.

If we have $L$ observations from a multinomial distribution, then using a
multinomial logit-link, with model matricies $M^1,\dots,M^L$, regression
parameter $\beta$, observed counts $Y^1,\dots,Y^N$ with observed sample
sizes $N^1,\dots,N^L$ and prior probabilities $\xi^1, \dots, \xi^L$ and
prior ``sample sizes'' $\nu^1,\dots,\nu^L$ then the posterior distribution
of $\beta$ is given by \citep{johnson-thesis}[Sec. 5.1.2]
\begin{equation}\label{eq:mult-post-conj-complicated}
\pi(\beta|y,n,\xi,\nu) \propto \exp\biggl\{ \sum_{l=1}^L \inner{y^l + \xi^l
    \nu^l, M^l \beta} - (n^l + \nu^l) \log\bigl(
    \sum_j e^{M_{j\cdot} \beta} \bigr) \biggr\}
\end{equation}
where $\inner{a, b}$ denotes the usual inner product between vectors $a$
and $b$.  For our application, we can simplify this in two ways.

First, we use the posterior counts instead of the sum of the prior and data
counts, i.e. use $y^{*l} = y^l + \xi^l \nu^l$ and $n^{*l} = n^l + \nu^l$.

Second, to avoid having a direction of recession in $\pi(\beta|\cdot)$, we
need to fix the elements of $\beta$ that correspond with one of the
response categories.  Since we are going to fitting a binomial response, if
we set these elements of $\beta$ to be $0$, we may then replace the
sequence of model matricies with a single model matrix; $M$ instead of
$M^1,\dots,M^L$.  The $l^{th}$ row of $M$ will correspond to $M^l$.  Label
the two response categories $A$ and $B$.  Without loss of generality, we
will fix the elements of $\beta$ corresponding to category $B$ to 0.

Let $x_1,\dots,x_L$ represent the posterior counts of category $A$, and
$\beta^*$ represent the corresponding elements of $\beta$ --- these are the
elements of $\beta$ we did not fix as 0.  The meaning of
$n^{*1},\dots,n^{*L}$ is unchanged.  Then our simplified un-nomralized
posterior density is
\begin{equation}\label{eq:simplified-posterior}
  \pi(\beta|x,n^*) \propto
  \exp\biggl\{
    \inner{x, M \beta^*} 
    - 
    \sum_{l=1}^L n^{*l} \log\bigl(1 + e^{(M \beta^*)_l}\bigr)
  \biggr\}.
\end{equation}
This can be computed with a very simple \texttt{R} function, we implement
it in log form.
<<def-posterior-binom>>=
lud.binom <- function(beta, M, x, n) {
  MB <- M %*% beta
  sum(x * MB) - sum(n * log(1 + exp(MB)))
}
@ 

Now that we have a function to calculate a log-unnormalized posterior
density, we can run the markov chain.  To that, we need the model matrix.
First we convert the \texttt{UCAdmissions} data to a \texttt{data.frame}.
<<convert>>=
dat <- as.data.frame(UCBAdmissions)
dat.split <- split(dat, dat$Admit)
dat.split <- lapply(dat.split,
                    function(d) {
                      val <- as.character(d$Admit[1])
                      d["Admit"] <- NULL
                      names(d)[names(d) == "Freq"] <- val
                      d
                    })
dat <- merge(dat.split[[1]], dat.split[[2]])
@ 

Next we build the model matrix.  Our model specification allows for an
interaction between gender and departement, even though our prior assumes
that they are independent.
<<build-model-matrix>>=
formula <- cbind(Admitted, Rejected) ~ (Gender + Dept)^2
mf <- model.frame(formula, dat)
M <- model.matrix(formula, mf)
@ 

As stated above, we will take $\nu = 5$ and $\xi=0.30$.  That is, we will
add 5 students to each gender-department combination, where each
combination has a 30\% acceptance rate.
<<>>=
xi <- 0.30
nu <- 5
@ 

<<lud-binom>>=
lud.berkeley <- function(B) 
  lud.binom(B, M, dat$Admitted + xi * nu, dat$Admitted + dat$Rejected + nu)
@ 

This function is suitable for passing to \texttt{metrop} or
\texttt{morph.metrop}.  We know that using \texttt{morph.metrop} with 
\texttt{morph=morph(p=3)} will run a geometrically ergodic Markov chain
\citep{johnson-geyer}.
<<>>=
berkeley.out <- morph.metrop(lud.berkeley, rep(0, ncol(M)), blen=1000, 
                             nbatch=1, scale=0.1, morph=morph(p=3))
berkeley.out$accept
berkeley.out <- morph.metrop(berkeley.out, scale=0.05)
berkeley.out$accept
berkeley.out <- morph.metrop(berkeley.out, scale=0.02)
berkeley.out$accept
berkeley.out <- morph.metrop(berkeley.out, blen=10000)
berkeley.out$accept
@ 

<<>>=
berkeley.out <- morph.metrop(berkeley.out, blen=1, nbatch=100000)
@ 

Estimate the posterior mean acceptance probabilities for each
gender-department combination.
<<>>=
beta <- setNames(colMeans(berkeley.out$batch), colnames(M))
MB <- M %*% beta
dat$p <- dat$Admitted / (dat$Admitted + dat$Rejected)
dat$p.post <- exp(MB) / (1 + exp(MB))
dat
@ 
The small difference between the data and posterior probabilities is
expected, our prior was given very little weight.  Using
\texttt{morph.metrop} with the setting \texttt{morph=morph(p=3)} in this
setting is an efficient way of sampling from the posterior distribution.

\begin{thebibliography}{}

\bibitem[Gelman et al.(1996)Gelman, Roberts, and Gilks]{grg}
Gelman, A., G.~O. Roberts, and W.~R. Gilks (1996).
\newblock Efficient Metropolis jumping rules.
\newblock In \emph{Bayesian Statistics, 5 (Alicante, 1994)}, pp.~599--607.
  Oxford University Press.

\bibitem[Geyer(1992)]{practical}
Geyer, C.~J. (1992).
\newblock Practical Markov chain Monte Carlo (with discussion).
\newblock \emph{Statistical Science}, 7, 473--511.

\bibitem[Geyer and Thompson(1995)]{geyer-temp}
Geyer, C.~J. and E.~A. Thompson (1995).
\newblock Annealing Markov chain Monte Carlo with applications to
    ancestral inference.
\newblock \emph{Journal of the American Statistical Association}, 90,
909--920.

\bibitem[Jarner and Roberts(2007)]{jarner-roberts}
Jarner, S.F. and G.O. Roberts (2007).
\newblock {C}onvergence of {H}eavy-tailed {M}onte {C}arlo {M}arkov {C}hain
  Algorithms.
\newblock \emph{Scandinavian Journal of Statistics}, 34, 781--815.

\bibitem[Johnson(2011)]{johnson-thesis}
Johnson, L.~T. (2011).
\newblock Geometric Ergodicity of a Random-Walk Metropolis Algorithm via
  Variable Transformation and Computer Aided Reasoning in Statistics.
\newblock \emph{University of Minesota}.

\bibitem[Johnson and Geyer(submitted)]{johnson-geyer}
Johnson, L.~T. and C.~J. Geyer (submitted).
\newblock Variable Transformation to Obtain Geometric Ergodicity
    in the Random-walk Metropolis Algorithm.
\newblock \emph{Annals of Statistics}.

\bibitem[Mengersen and Tweedie(1996)]{mengersen-tweedie}
  Mengersen, K.L. ad R. L. Tweedie (1996).
\newblock Rates of convergence of the {H}astings and {M}etropolis algorithms.
\newblock \emph{Annals of Statistics}, 24, 101--121.

\end{thebibliography}

\end{document}
