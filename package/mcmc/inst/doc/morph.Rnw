\documentclass{article}

\usepackage{natbib}
\usepackage{graphics}
\usepackage{amsmath, amssymb}
\usepackage{indentfirst}
\usepackage[utf8]{inputenc}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\E}{E}

% \VignetteIndexEntry{MCMC Morph Example}

\begin{document}

<<foo,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
foo <- packageDescription("mcmc")
@

\title{Morphemetric MCMC Example (mcmc Version \Sexpr{foo$Version})}
% $ (Just to make emacs syntax highlighting work properly)
\author{Leif T. Johnson \\ and Charles J. Geyer}
\maketitle

\section{Overview}

This is an example how to use morphemetric Markov chains as implemented in
the \verb@mcmc@ package in R.

Let $X$ be a $\mathbb{R}^k$ valued random variable with probability density
function (pdf), $f_X$.  Let $g$ be a diffeomorphism, and $Y=g(X)$.  Then the 
pdf of $Y$, $f_Y$ is given by the equation
\begin{equation}\label{eq:def-fy}
  f_Y(y) = f_X\bigl(g^{-1}(y)\bigr) | \nabla g^{-1}(y) |.
\end{equation}
Since $g$ is a diffeomorphism, we can draw inference about $X$ from information 
about $Y$ (and vice versa).  

It is not unusual for $f_X$ to either be known only up to a normalizing 
constant, or to be analytically intractable in other ways --- such as a large 
number of dimensions.  A common solution to this problem is to use Markov chain
Monte Carlo (MCMC) methods to learn about $f_X$ when this happens.

When using MCMC, a primary concern of the practicioner should be the question 
"Does the Markov chain converge fast enough to be useful?"  One very useful 
convergence rate is called \emph{geometrically ergodic} \textbf{CITE SOMETHING 
HERE}.

The \texttt{mcmc} package implements the Metropolis random-walk algorithm for 
arbitrary log unnormalized probability densities.  But the Metropolis 
random-walk algorithm does not always perform well.  As is demonstrated in 
\citet{johnson-geyer}, for one-to-one densities, $f_X$ and $f_Y$ defined as in
\eqref{eq:def-fy} a Metropolis random-walk $f_Y$ can be geometrically ergodic 
even though a Metropolis random-walk for $f_X$ is not.  Since the densities are
one-to-one, inference about $f_X$ can be drawn from the Markov chain for $f_Y$.

The \texttt{morph.metrop} and \texttt{morph} functions in the \texttt{mcmc} 
package provide this functionality, and this vignette gives a demonstration
on how to use them.

\section{$T$ Distribution}

\citet{johnson-geyer}[Example 4.2] show that a T distribution is
sub-exponentially light, and hence a Metropolis random-walk for a $T$
density cannot be geometrically ergodic, but, using the transformations
described in their Corollaries 1 and 2 will induce a target density,
$\pi_\gamma$ for which a Metropolis random-walk will be geometrically
ergodic.  In fact, we know from \citet{mengersen-tweedie} that for a
univariate target density, being exponentially light is a sufficient
condition for the geometric ergodicity of a Metropolis random-walk.  Hence
using the transformation described as $h_2$ in
\citet{johnson-geyer}[Corollary 2] will induce a target density for which a
Metroplis random-walk will be geometrically ergodic.  This section
demonstrates how to do this using the \texttt{mcmc} package for
\texttt{R}.  We will use a univariate $T$ distribution with 3 degrees of
freedom for a target density.

Passing a positive value for \texttt{b} to \texttt{morph} function will
create the aforementioned transformation, $h_2$.  It's as simple as 
<<>>=
library(mcmc)
h2 <- morph(b=1)
@ 
We can now see the induced density.  Note that \texttt{morph} works for
log unnormalized densities, so we need exponentiate the induced density to
plot it on the usual scale.
<<>>=
lud <- function(x) dt(x, df=3, log=TRUE)
lud.induced <- h2$lud(lud)
@ 
We can plot the two densities,
<<fig=TRUE>>=
x <- seq(-3, 3, length.out=400)
plot(x, exp(sapply(x, lud.induced)), lty=2, type="l",
     xlab="t", ylab="")
lines(x, exp(lud(x)), lty=1)
legend("topright", c("t-density", "induced density"), lty=1:2)
@ 
The \texttt{sapply} in this example is necessary, \texttt{lud.induced} is not 
vectorized.  Instead, it treats any vector passed as a single input, which
is rescaled (using an isomorphic transformation) and passed to
\texttt{lud}.  Compare the behavior of \texttt{lud} and
\texttt{lud.induced} in the following example.
<<>>=
lud(rep(0, 4))
lud(1)
lud.induced(rep(0, 4))
lud.induced(1)
@
Because \texttt{dt} is vectorized, \texttt{lud} is vectorized so it maps 
vectors to vectors, but \texttt{lud.induced} treats the vector as a single
input, which is transformed and passed to \texttt{lud}.

Running a Markov chain for the induced density is done with
\texttt{morph.metrop}.
<<>>=
out <- morph.metrop(lud, 0, blen=100, nbatch=100, morph=morph(b=1))
@ 
The content of \texttt{out\$batch} is on the scale of used by
\texttt{lud}.  Once the transformation has been set, no adjustment is
needed (unless you want to change transformations).  We start by adjusting
the scale.
<<>>=
# adjust scale to find a roughly 20% acceptance rate
out$accept
@ 
An acceptance rate of \Sexpr{round(100 * out$accept, 1)}\%
%$ to fix emacs highlighting
is probably too high.  By increasing the scale of the proposal distribution
we can bring it down towards 20\%.
<<>>=
out <- morph.metrop(out, scale=4)
out$accept
@ 
We now use this Markov chain to estimate 
\begin{equation}\label{eq:e-abs-t}
  \E(|T|)
\end{equation}
for the $T$ distribution with 3 degrees of freedom.  We'll start with an
estimate from 1000 iterations of the chain.  
<<>>=
out <- morph.metrop(out, blen=1, nbatch=1e4, outfun=abs)
@ 
We're going to use over-lapping batch means to estimate \eqref{eq:e-abs-t},
but first we need to establish a reasonable batch length.
<<fig=TRUE>>=
acf(out$batch, lag.max=200)
@ 
It looks like a batch length of 100 will be sufficiently long.
<<>>=
mcse <- sqrt(olbm(out$batch, 100))
est.mean <- colMeans(out$batch)
est.mean
mcse
@ 
The variable \texttt{est.mean} gives our MCMC estimate of
\eqref{eq:e-abs-t} and \texttt{mcse} gives the Monte Carlo standard error
of our estimate.  Because the underlying Markov chain is geometrically
ergodic and $\E(|T|^2)$ is finite, there is a central limit theorem for our
estimate.  The confidence interval for \eqref{eq:e-abs-t} is given by
<<>>=
est.mean + mcse * 1.96 * c(-1,1)
@ 
If a narrower confidence interval is desired, the Markov chain can be run
longer.

Note that when calculating our estimate and the Monte Carlo standard error
we are not concerned with what was happening on the transformed scale.  The
\texttt{morph.metrop} function seamlessly does this for us.

\subsection{Simulation Study}

<<>>=
n.sims <- 100
n.iter <- 5000
@ 

To show the utility of the transformation, we will study the distribution
of estimates of $\E(|T|)$ for a univariate $T$ random variable with three
degrees of freedom.  We will consider two different estimation methods:
\begin{enumerate}
\item \label{enum:rw} We will estimate $\E(|T|)$ using a random-walk Metropolis algorithm
for the $T$ density by using the \texttt{metrop} function.
\citet{jarner-roberts} demonstrate that a central limit theorem does not
hold for these estimates.
\item \label{enum:rw-induced} We will estimate $\E(|T|)$ using a
  random-walk Metroplis algorithm for the density induced by setting
  \texttt{b=1} in the \texttt{morph.metrop} function.
  \citet{johnson-geyer} demonstrate that a central limit does hold for
  these estimates.
\end{enumerate}

To demonstrate this we will estimate $\E(|T|)$ \Sexpr{n.sims} times with
each method, in each case using a Markov chain \Sexpr{n.iter} iterations
long, and then examine the normality of the resulting estimates.

We have already determined that for the induced density, a scale of 4 is
about right.  Before doing the simulations we will establish a reasonable
scale to use for the random-walk Metropolis altorithm run for the $T$
density.

<<>>=
out <- metrop(lud, 0, blen=1000, nbatch=1)
out$accept
out <- metrop(out, scale=4)
out$accept
out <- metrop(out, scale=6)
out$accept
@ 
A scale of 6 appears to be about right.  It's now easy to run these
simulations.

<<runsims>>=
t.estimates <- replicate(n.iter, metrop(lud, 0, blen=1000, nbatch=1,
                                        outfun=abs)$batch)
induced.estimates <- replicate(n.iter, morph.metrop(lud, 0, blen=1000, nbatch=1,
                                                    outfun=abs, morph=morph(b=1))$batch)

@ 

Now we can look at the distribution of our estimates.  Since this is a
``toy'' example, we can establish a truth value using Good Old Fashioned
Monte Carlo (GOFMC).

<<establishtruth>>=
n.truth <- 1000000
t.values <- abs(rt(n.truth, df=3))
truth <- mean(t.values)
truth.se <- 1.96 * sd(t.values) / sqrt(n.truth)
truth
truth.se
@
This estimates the true value out to $\pm \Sexpr{signif(truth.se, 1)}$.

First, we look at histograms of the estimates.

<<fig=TRUE>>=
par(mfrow=c(1,2))
hist(t.estimates, xlab="", main="Estimates from R-W on T")
lines(rep(truth, 2), c(0, length(t.estimates)), col="red")
lines(rep(mean(t.estimates), 2), c(0, length(t.estimates)), lty=2)
hist(induced.estimates, xlab="",
     main="Estimates from R-W\non Induced Distribution")
lines(rep(truth, 2), c(0, length(induced.estimates)), col="red")
lines(rep(mean(induced.estimates), 2), c(0, length(induced.estimates)), lty=2)
@ 

Notice that the distribution coming from the estimates based on the
random-walk Metropolis algorithm for the induced
density are symmetric and follow a normal distribution, but the estimates
based on the random-walk Metropolis algorithm for the $T$ density do not
follow a symmetric distribution.

The following Q-Q plots for the standardized estimates support this theory.

<<fig=TRUE>>=
par(mfrow=c(1,2))
t.standardized <- t.estimates - mean(t.estimates) / sd(t.estimates)
induced.standardized <- induced.estimates - mean(induced.estimates) / sd(induced.estimates)
qqnorm(t.standardized, main="Q-Q Plot\nEstimates from T")
qqline(t.standardized)
qqnorm(induced.standardized, main="Q-Q Plot\nEstimates from Induced Density")
qqline(induced.standardized)
@ 

Finally, we can examine the normality of the estimates with a
Komogorov-Smirnov test.

<<>>=
t.kstest <- ks.test(t.standardized, pnorm)
induced.kstest <- ks.test(induced.standardized, pnorm)
t.kstest$p.value
induced.kstest$p.value
@ 

\section{Binomial Distribution with a Conjugate Prior}

We demonstrate a morphemetric Markov chain using the \textttt{UCBAdmisions}
data set included in \texttt{R}, (use \texttt{help(UCBAdmissions)} to see
details of this data set).  We will model the probability of a student
being admitted or rejected, using the sex of the student and the department
that the student applied to as predictor variables.  For our prior, we
naively assume that 30\% of all students are admitted, independent of sex
or department.  As this is a naive prior, we will not give it much weight
and allow most of the information in the posterior distribution to come
from the data.

First we will convert the data to a more convenient format.  The
\texttt{UCAdmissions} data is stored in a table format, we convert it to a
\texttt{data.frame}.
<<convert>>=
dat <- as.data.frame(UCBAdmissions)
dat.split <- split(dat, dat$Admit)
dat.split <- lapply(dat.split,
                    function(d) {
                      val <- as.character(d$Admit[1])
                      d["Admit"] <- NULL
                      names(d)[names(d) == "Freq"] <- val
                      d
                    })
dat <- merge(dat.split[[1]], dat.split[[2]])
@ 

Next we build the model matrix.
<<>>=
formula <- cbind(Admitted, Rejected) ~ (Sex + Dept)^2
utils::str(m <- model.frame(formula, dat))
mat <- model.matrix(ff, m)
@ 

\begin{thebibliography}{}

\bibitem[Gelman et al.(1996)Gelman, Roberts, and Gilks]{grg}
Gelman, A., G.~O. Roberts, and W.~R. Gilks (1996).
\newblock Efficient Metropolis jumping rules.
\newblock In \emph{Bayesian Statistics, 5 (Alicante, 1994)}, pp.~599--607.
  Oxford University Press.

\bibitem[Geyer(1992)]{practical}
Geyer, C.~J. (1992).
\newblock Practical Markov chain Monte Carlo (with discussion).
\newblock \emph{Statistical Science}, 7, 473--511.

\bibitem[Geyer and Thompson(1995)]{geyer-temp}
Geyer, C.~J. and E.~A. Thompson (1995).
\newblock Annealing Markov chain Monte Carlo with applications to
    ancestral inference.
\newblock \emph{Journal of the American Statistical Association}, 90,
909--920.

\bibitem[Jarner and Roberts(2007)]{jarner-roberts}
Jarner, S.F. and G.O. Roberts (2007).
\newblock {C}onvergence of {H}eavy-tailed {M}onte {C}arlo {M}arkov {C}hain
  Algorithms.
\newblock \emph{Scandinavian Journal of Statistics}, 34, 781--815.

\bibitem[Johnson and Geyer(submitted)]{johnson-geyer}
Johnson, L.~T. and C.~J. Geyer (submitted).
\newblock Variable Transformation to Obtain Geometric Ergodicity
    in the Random-walk Metropolis Algorithm.
\newblock \emph{Annals of Statistics}.

\bibitem[Mengersen and Tweedie(1996)]{mengersen-tweedie}
  Mengersen, K.L. ad R. L. Tweedie (1996).
\newblock Rates of convergence of the {H}astings and {M}etropolis algorithms.
\newblock \emph{Annals of Statistics}, 24, 101--121.

\end{thebibliography}

\end{document}
