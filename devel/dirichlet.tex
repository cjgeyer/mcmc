
\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{indentfirst}
\usepackage{natbib}
\usepackage{url}

\DeclareMathOperator{\aff}{aff}

\newcommand{\real}{\mathbb{R}}

\newcommand{\set}[1]{\{\, #1 \,\}}

\begin{document}

\title{Sampling Constrained Dirichlet Distributions}

\author{Charles J. Geyer}

\maketitle

\section{Introduction}

\subsection{Problem Specification} \label{sec:specification}

Let $C$ denote the convex polytope that consists of all points $x$ in
$d$-dimensional Euclidean space satisfying the constraints
\begin{enumerate}
\item[(i)] $x \ge 0$,
\item[(ii)] $\sum_{i = 1}^d x_i = 1$,
\item[(iii)] $A_1 x \le b_1$, and
\item[(iv)] $A_2 x = b_2$,
\end{enumerate}
where the inequalities work componentwise, as in R \citep{r},
where $A_1$ is a matrix
and $b_1$ is a vector having dimensions such that (iii) makes sense and
similarly for $A_2$ and $b_2$, and where either or both of (iii) and (iv)
may be absent.  Let $\alpha$ be a vector having positive-real-valued
components.  We call the probability distribution having unnormalized density
\begin{equation} \label{eq:unnormalized-density}
   h(x) = \prod_{i = 1}^d x_i^{\alpha_i - 1}
\end{equation}
with respect to Lebesgue measure on $C$ the \emph{constrained Dirichlet
distribution} on $C$ having parameter vector $\alpha$.
By Lebesgue measure on $C$ we mean Lebesgue measure on the affine hull
of $C$ restricted to $C$ (more on this in Section~\ref{sec:lebesgue} below).

This is the problem specified by the user.
We discuss efficient Markov chain sampling of such distributions.

\subsection{Hit-and-Run Samplers}

The R package \texttt{polyapost} \citep{polyapost} already does this
for the special case when the Dirichlet distribution is uniform
($\alpha_i = 1$ for all $i$), and we will use some
of their ideas, in particular, the hit-and-run sampler
\citep*{smith,hit-run-one,hit-run-two}, but we will also use some other ideas.
The R package \texttt{hitandrun} \citep{hitandrun}
also does the uniform distribution on a
convex polytope using a hit-and-run sampler (competing with \texttt{polyapost}).

There do not appear to be any R packages on CRAN that do constrained Dirichlet
distribution sampling, other than general purpose MCMC packages like
the R package \texttt{mcmc} \citep{mcmc}, which uses a Metropolis
random-walk sampler (\citealp{metropolis-et-al};
\citealp[Section~2.3.2]{tierney})
and simulates any distribution specified by a user
supplied function.  We do not use this because we want to make a sampler
specifically tuned to the constrained Dirichlet problem.

\subsection{Computational Geometry}

We will use the R package \texttt{rcdd} \citep*{rcdd} to handle
the computational geometry of the constraint set $C$
(more on this in Section~\ref{sec:computational-geometry} below).

\subsection{Asymptotic Approximation}

Define the vector $\lambda$ having components
\begin{equation} \label{eq:lambda}
   \lambda_i = \frac{\alpha_i}{\alpha_1 + \cdots + \alpha_d}
\end{equation}
and let $\Lambda$ denote the diagonal matrix having the vector $\lambda$
as its diagonal.  Then the normal approximation to the constrained
Dirichlet distribution is
the $\text{Normal}(\lambda, \Lambda)$ distribution
restricted to $C$ \citep[Theorem~4.2]{geyer-meeden}
(more on this in Section~\ref{sec:approximate} below).

For $\alpha$ having
all components ``large'' this will be a very good approximation, so we want
to use this to speed up the simulation.  Otherwise, this may be a bad
approximation, so we want to use the approximation in a way that still
allows simulation of the exact distribution of interest.

\subsection{Independence Samplers}

Independence updates (\citealp{hastings};
\citealp[Section~2.3.3]{tierney}) use the asymptotic approximation
to simulate the exact distribution of interest.

In detail, propose $y$ from this constrained normal distribution,
then do Metropolis-Hastings rejection: calculate
$$
    R = \frac{f(y) \varphi(x)}{f(x) \varphi(y)}
$$
where $\varphi$ is the density of the normal proposal and accept the
proposal with probability $\min(1, R)$.
We combine hit-and-run and independence updates to make a hybrid sampler
\citep[Section~2.4]{tierney} better than either update used by itself.

\section{Computational Geometry} \label{sec:computational-geometry}

A convex polytope like our constraint set $C$ has two different
representations, as the set satisfying a finite set of linear equalities
and inequalities, like those in the list in Section~\ref{sec:specification},
and as the convex hull
of the vertices of the set.   The R package \texttt{rcdd} calls these
the H-representation and the V-representation
(H for half space and V for vertex).  The function
\texttt{scdd} in that package goes between them.

Modify the definition of $A_1$, $b_1$, $A_2$, and $b_2$ in the list in
Section~\ref{sec:specification}
so that $A_1$ and $b_1$ include the nonnegativity constraints (i) and
$A_2$ and $b_2$ include the sum to one constraint (ii).  Then the
R function \texttt{makeH} in the \texttt{rcdd} package makes
an H-representation for $C$.

\subsection{Vertices}

Apply the R function \texttt{scdd} to the H-representation.
It produces the V-representation which gives the set of vertices of $C$.
Call this vertex set $V$.

If $V$ is empty or a singleton set, we return an error to the user.
If $V$ is empty, then $C$ is empty too, so the user has indeed made an
error in specifying inconsistent constraints.  There is no probability
distribution on the empty set.
If $V$ is a singleton set, then $C = V$, so while, strictly speaking,
the user has not made an error, the probability distribution is trivial.
There is only one probability distribution on a one-point sample space,
and we do not need MCMC to describe it.
Thus in what follows we assume $C$ has at least two vertices.

Let $x_V$ denote any convex combination of the points in $V$.
This is a point known to be in $C$, which we can use as the starting
point for our MCMC sampler.

\subsection{Affine Hull}

The first issue we have to deal with is that $C$ is not $d$-dimensional.
Since we cannot deal with a distribution that is singular with respect to
Lebesgue measure, we must reduce the dimension.  The \emph{affine hull}
of $C$, denoted $\aff C$, is the smallest affine space (translate of a
vector subspace) that contains $C$.  By definition, the dimension of $C$
is the dimension of $\aff C$.

In order to sample effectively we must ``find'' $\aff C$, which means
we need to find an invertible linear map from $\real^p$ to $\aff C$.

\subsubsection{Redundancy}

What is the dimension of $C$?  We can't tell from our H-representation,
which may have hidden redundancies.  The R function \texttt{redundant}
in the \texttt{rcdd} package produces an equivalent H-representation that
is non-redundant (the equality constraints are linearly independent,
and the inequality constraints are true inequalities rather than equalities,
that is, they actually hold with strict inequality for some points in $C$).

Now from the non-redundant H-representation, we know that the dimension of
$C$ is $p = d - l$, where $l$ is the number of equality constraints in the
non-redundant H-representation.  We know that $p \ge 1$ from our check that
$C$ has at least two vertices (preceding section).

\subsubsection{Map}

In this section we only need the equality constraints from
this non-redundant H-representation.  Extract its rows
representing equality constraints, giving yet another H-representation,
which represents $\aff C$.

Apply the R function \texttt{scdd} to the H-representation of $\aff C$.
It produces the V-representation of $\aff C$, which (unlike the
V-representation of a polytope) contains both points and directions.
In this case there will be exactly one point and one or more directions
(this follows from our assumption that $C$ has at least two vertices,
which implies that the dimension of $\aff C$ is at least one and from
our assumption that $C$ is contained in the unit simplex so it does
not contain the origin, $\aff C$ is not a vector space but rather a
translate of a vector space, so there must be one point in the
V-representation, which is the translation).

Let $M$ denote the matrix whose columns are the directions in the
V-representation of $\aff C$ so the column space of $M$ is the vector
subspace of $\real^d$ that is parallel to $\aff C$.  Let $p$ be the
column dimension of $M$ (the number of directions in the V-representation
of $\aff C$).  Then
\begin{equation} \label{eq:map}
   w \mapsto x_V + M w
\end{equation}
is an isomorphism between $\real^p$ and $\aff C$ (it is one-to-one and onto).
We think of \eqref{eq:map} as providing a coordinatization of $\aff C$.
We also say that \eqref{eq:map} goes from the new coordinates (NC) to
the original coordinates (OC).  Our sampler runs in NC.  The output
must be mapped via \eqref{eq:map} back to OC for presentation to the user
(the OC are the only coordinates the user knows about).

\subsection{Constraint Set in New Coordinates}

In order to finish our description of $C$ we need to describe the inequality
constraints in our new coordinates.  Let $A x \le b$ denote the inequality
constraints in the original coordinates, which combine items (i) and (iii)
in the list in Section~\ref{sec:specification}.
In NC these constraints are $A x_V + A M w \le b_1$,
and we denote this $A_3 w \le b_3$ where
\begin{align*}
   A_3 & = A M
   \\
   b_3 & = b - A x_V
\end{align*}
Define
$$
   C_3 = \set{ w \in \real^p : A_3 w \le b_3 }.
$$
In NC there are no equality constraints (that is the point of NC).
$C_3$ is the constraint set in NC.
Let $V_3$ denote the set of vertices of $C_3$.
The mapping \eqref{eq:map} maps $V_3$ one-to-one and onto $V$.

As in this section, we will use subscript 3 to mean NC throughout
this whole document.

\section{The Distribution}

\subsection{Lebesgue Measure} \label{sec:lebesgue}

The map \eqref{eq:map} maps Lebesgue measure on $\real^p$ to Lebesgue measure
on $\aff C$.  Different choices of $M$ (only the column space of $M$ matters,
not its actual components) give different notions of Lebesgue measure on
$\aff C$, but since the Jacobian of a linear transformation is constant,
these different notions of Lebesgue measure are scalar multiples of each other.
Since we are using unnormalized densities, it does not matter which choice
of Lebesgue measure on $\aff C$ (which choice of $M$) is used.
We always get the same distribution after normalization.

\subsection{Exact}

We now need to map the unnormalized density \eqref{eq:unnormalized-density}
to the new coordinates.  Define $h_3$ by
\begin{equation} \label{eq:h-new}
   h_3(w) = h(x_V + M w), \qquad w \in C_3.
\end{equation}
Our problem is now to sample the distribution concentrated on $C_3$
having unnormalized density $h_3$ with respect to Lebesgue measure.

\subsection{Approximate} \label{sec:approximate}

The normal approximation to the Dirichlet distribution with
parameter vector $\alpha$ has unnormalized density
$$
   h_{\text{approx}}(x)
   =
   \exp\left(- \tfrac{1}{2} (x - \lambda)^T \Lambda^{- 1} (x - \lambda) \right)
$$
where $\lambda$ is given by \eqref{eq:lambda} and $\Lambda$ is diagonal
with $\lambda$ on the diagonal \citep[Theorem~4.2]{geyer-meeden}.

We need to move this to the new coordinates.  Define
\begin{equation} \label{eq:h-approx-new}
   h_4(w)
   =
   \exp\left(- \tfrac{1}{2} (x_V + M w - \lambda)^T
   \Lambda^{- 1} (x_V + M w - \lambda) \right).
\end{equation}
The definition of $h_4$ given above is perfectly good for running
Metropolis-Hastings updates to sample it.  But we also want to
be able to generate independent samples from this normal distribution.
And for that we need to know its mean vector and variance matrix.
Let them be denoted $\mu$ and $\Sigma$.  Then we have
$$
   (x_V + M w - \lambda)^T \Lambda^{- 1} (x_V + M w - \lambda)
   =
   (w - \mu)^T \Sigma^{- 1} (w - \mu) + \text{constant}
$$
from which we see that
\begin{equation} \label{eq:h-approx-new-variance}
   \Sigma^{- 1} = M^T \Lambda^{- 1} M
\end{equation}
and
$$
   M^T \Lambda^{- 1} (\lambda - x_V) = \Sigma^{- 1} \mu
$$
so
\begin{equation} \label{eq:h-approx-new-mean}
   \mu = \left( M^T \Lambda^{- 1} M \right)^{- 1}
   M^T \Lambda^{- 1} (\lambda - x_V)
\end{equation}
\citep[compare equation (25) in][]{geyer-meeden}.

\section{Sampler}

\subsection{Hit-and-Run Updates}

The basic idea of a hit-and-run update is to make a move along a randomly
chosen direction.    If $w \in C_3$ is the current position of the Markov
chain, choose a random direction $d \in \real^p$ and then propose to move
to $z = w + s d$, where $s$ is any real number such that $z \in C_3$.
The proposal is then Metropolis rejected so update preserves the desired
stationary distribution.

Hit-and-run samplers differ in how the random direction is chosen and in
how the proposal is made given the chosen direction.  So we have two
different issues to consider.

\subsubsection{Choosing the Direction}

The conventional way to choose a random direction is to generate
a random vector from a spherically symmetric normal distribution.
The R packages \texttt{polyapost} and \texttt{hitandrun} do this.
This choice is very problematic, however because it has no relation
to the problem at hand.  We see this in the map \eqref{eq:map} which
is arbitrary (recall that only the column space of $M$ matters) but
which affects how the unit ball in $\real^p$ maps into $\aff C$,
which is what really matters.  (Spherical symmetry in $\aff C$ is not
even well defined, but however one might define it, the definition
should not be arbitrary, and the map \eqref{eq:map} is arbitrary,
in the sense that if $M$ is changed but its column space is not changed,
then the map works just as well.)
Moreover, this method pays no attention
to the inequality constraints (specified by $A_3$ and $b_3$) nor any
attention to the unnormalized density $h_3$.

There is no reason to follow this convention.  Any method of choosing
a random direction will do.  \citet{smith} and \citet{hit-run-one}
discuss choosing the random direction to be along one of the coordinate axes,
so the directions are chosen from a discrete set, not a continuum.
But even more generality is possible.
\citet[Section~1.12.8]{geyer-intro} points out
that any state-independent mixture of updates that preserve the desired
stationary distribution also preserves that distribution.  In this context
that means any distribution on directions whatsoever works, so long as the
distribution on directions is fixed (it does not change from iteration
to iteration and does not depend on the state of the Markov chain).

We want a proposal that pays attention to the problem at hand, in particular,
to the convex polytope $C_3$ that is the constraint set in our new coordinates.
Let $V_3$ denote the set of vertices of $C_3$.  These can be found either
by mapping the points of $V$ (vertices in old coordinates) through the
inverse mapping of \eqref{eq:map} or by running the R function \texttt{scdd}
on the H-representation given by $A_3$ and $b_3$.  Every ordered pair of
distinct points of $V_3$ determines a direction (from the first point to the
second point).  We propose to choose hit-and-run directions from this
(finite) set of directions.  There is no need to choose these directions
with equal probability; it seems more reasonable to choose so that directions
between vertices that are farther apart are chosen with higher probability
(the idea being that the main thing that makes a hit-and-run sampler mix
slowly is difficulty finding directions in the polytope in which long moves
can be made).  Exactly what nonuniform distribution should be used is an
open question for which we have no good argument.  For now we propose
(without having given the subject much thought) to have the probability
of a direction proportional to the distance between the vertices that
specify the direction.

Since we are now proposing to sample a fairly complicated discrete
distribution, how do we do that?  An efficient method is Walker's method
of aliases \citep{walker}.  Rather than bother to implement it ourselves,
we propose to steal the code
in the C function \verb@walker_ProbSampleReplace@ in the file
\texttt{src/main/random.c} in the current R distribution \citep{r},
which is 3.1.0 at the time of this writing.  This is the function R
itself uses to implement the R function \texttt{sample} when sampling
with replacement is requested and the number of samples requested
is greater than 200.  We would have to break up the function
\verb@walker_ProbSampleReplace@ in R into two functions, which is
easy to do, since the two parts are commented.  The first part creates
the alias tables, the second part generates samples, and both parts
are commented as such.  We would generate the alias tables exactly once,
and generate one ``sample'' for each hit-and-run update we do.
There is no problem in our stealing this code because R is free software
(GPL version 2 or any later version) so we only need use a GPL-compatible
licence for our software, which is required to be a CRAN package anyway.

\subsubsection{Proposal given the Direction} \label{sec:proposal-uniform}

Given the direction, we need to propose a point on the line
\begin{equation} \label{eq:line}
   L = \set{ w + s d : s \in \real }
\end{equation}
where $w$ is the current point and $d$ is the chosen direction.  Again
there is a standard proposal, and it is uniform on the part of the line
$L \cap C_3$ that is in the constraint set.

This uniform proposal has the virtue of being a Metropolis proposal,
so one uses the odds ratio
\begin{equation} \label{eq:metropolis-ratio}
   R = \frac{h_3(y)}{h_3(w)}
\end{equation}
where $w$ is the current state of the Markov chain and $y$ is the proposal
uniformly distributed on $L \cap C_3$, to do Metropolis rejection: accept
the proposal with probability $\min(1, R)$.

The next section generalizes this proposal.

\subsubsection{Yet Another Proposal given the Direction}

We are concerned that the hit-and-run proposals described in the
preceding section will work well when some of the components of the
parameter vector $\alpha$ of the Dirichlet distribution are near zero nor
will it work well when some of the components of $\alpha$ are very large.
More generally, we are concerned that the hit-and-run proposals described
in the preceding section pay no attention to the desired stationary
distribution having unnormalized density $h_3$.

So in this section we concoct a proposal that does pay (some attention)
to $h_3$.  Again let the current point be $w$ and the direction chosen $d$.
Let $s_\text{low}$ and $s_\text{hig}$ be such that the segment of the line
\eqref{eq:line} that is in $C_3$ is
$$
   \set{ w + s d : s_\text{low} < s < s_\text{hig} }
$$
and define
\begin{align*}
   w_\text{low} & = w + s_\text{low} d
   \\
   w_\text{hig} & = w + s_\text{hig} d
\end{align*}
The corresponding points in OC are
\begin{align*}
   x_\text{low} & = x_V + M w_\text{low}
   \\
   x_\text{hig} & = x_V + M w_\text{hig}
\end{align*}
In general, there will be exactly one inequality constraint satisfied with
equality at these points.
Another way of saying this is that $x_\text{low}$ and $x_\text{hig}$ will
have at most one component exactly equal to zero (ideally, if infinite
precision arithmetic had been used, in inexact computer arithmetic only
approximately zero).  It will be one component equal to zero if the constraint
involved was one of those in item (i) in the list
in Section~\ref{sec:specification}.  And it will be no components equal to zero
if the constraint involved was one of those in item (iii) in the list in
Section~\ref{sec:specification}.

Let $\alpha_\text{low}$ denote the
component of $\alpha$ corresponding to the component of $x_\text{low}$ that
is equal to zero, if there is one, and otherwise set $\alpha_\text{low} = 1$.
Similarly, for $\alpha_\text{hig}$.

We propose to use the proposal distribution having a transformed
beta distribution.  Let $v$ be a
$\text{Beta}(\alpha_\text{low}, \alpha_\text{hig})$ random variate.
Then we propose the point
\begin{equation} \label{eq:u-to-y}
   y = (1 - u) w_\text{low} + u w_\text{hig}
\end{equation}
The proposal density is
$$
   q_L(w, y)
   =
   \frac{\Gamma(\alpha_\text{low} + \alpha_\text{hig})}
   {\Gamma(\alpha_\text{low}) \Gamma(\alpha_\text{hig})}
   \cdot
   \frac{u^{\alpha_\text{low} - 1} (1 - u)^{\alpha_\text{hig} - 1}}
   {s_\text{hig} - s_\text{low}}
$$
In considering a Metropolis-Hastings update, we need to evaluate
the ratio of $q_L(w, y)$ and $q_L(y, w)$.  Clearly everything cancels
in the ratio except the part involving $u$.  If we solve \eqref{eq:u-to-y}
for $u$ we get
\begin{align*}
   u & = \frac{y - w_\text{low}}{w_\text{hig} - w_\text{low}}
   \\
   1 - u & = \frac{w_\text{hig} - y}{w_\text{hig} - w_\text{low}}
\end{align*}
and
$$
   \frac{q_L(y, w)}{q_L(w, y)}
   =
   \left( \frac{w - w_\text{low}}{y - w_\text{low}}
   \right)^{\alpha_\text{low} - 1}
   \left( \frac{w_\text{hig} - w}{w_\text{hig} - y}
   \right)^{\alpha_\text{hig} - 1}
$$
and the Hastings ratio is
\begin{equation} \label{eq:ratio-beta-hastings}
   R = \frac{h_3(y) q_L(y, w)}{h_3(w) q_L(w, y)}
   =
   \frac{h_3(y)}{h_3(w)}
   \left( \frac{w - w_\text{low}}{y - w_\text{low}}
   \right)^{\alpha_\text{low} - 1}
   \left( \frac{w_\text{hig} - w}{w_\text{hig} - y}
   \right)^{\alpha_\text{hig} - 1}
\end{equation}

\subsection{Independence Updates} \label{sec:independence}

All of the work in setting this up has already been described.  Simulate
a multivariate normal random vector $y$ in $\real^p$ having mean vector $\mu$
and variance matrix $\Sigma$ given by \eqref{eq:h-approx-new-mean}
and \eqref{eq:h-approx-new-variance}.  Calculate the Hastings ratio
\begin{equation} \label{eq:ratio-independence}
    R = \frac{h_3(y) h_4(x)}{h_3(x) h_4(y)}
\end{equation}
where $x$ is the current state and $h_3$ and $h_4$ are as described above
\eqref{eq:h-new} and \eqref{eq:h-approx-new}.  Accept this proposal
with probability $\min(1, R)$.

Note that all of these proposals are from exactly the same normal distribution.
Hence we need not do the set-up more than once.  One Cholesky decomposition
of $\Sigma$ suffices.  If $\Gamma \Gamma^T = \Sigma$,
then $z = \mu + \Gamma u$, has the desired normal distribution when $u$
is standard multivariate normal.

\section{Care about Computer Arithmetic}

All of this assumes exact computer arithmetic or, at least, does not
worry enough about the effects of inexact computer arithmetic.
So now we start worrying.

\subsection{Computational Geometry}

The R package \texttt{rcdd} that we are using for computational geometry
allows computation using exact infinite-precision rational arithmetic.
Thus we allow the user to specify the constraints, the matrices $A_1$ and
$A_2$ and the vectors $b_1$ and $b_2$ in the list
in Section~\ref{sec:specification}, using infinite-precision rational numbers.
These are specified using character strings like \verb@"2/3"@
or \verb@"-17/4"@ or \verb@"0"@.

Whether the user specifies these matrices using rational numbers or
the computer's usual numbers (which are inexact having only about 16
decimal places of precision), our computer code immediately converts
these matrices to rational numbers using the R function \texttt{d2q}
in the \texttt{rcdd} package.
All of the computations described in Section~\ref{sec:computational-geometry}
are then done using infinite-precision rational arithmetic.  Hence they
are exact.

Because the rest of R uses ordinary computer arithmetic, our computer code
must convert the vector $x_V$ and the matrix $M$ in the map \eqref{eq:map},
the vector $b_3$ and the matrix $A_3$ that express the inequality constraints
in NC, and the matrix $V_3$ whose rows are the vertices of the constraint
set $C_3$ in NC, all of which are originally computed in infinite-precision
rational arithmetic back to ordinary computer numbers using the R function
\texttt{q2d} in the \texttt{rcdd} package.

We then use ordinary computer arithmetic for the sampler.  This is essential
because we are using uniform, normal, and beta random variates that come
from the R random number generators and these use ordinary computer numbers.
It is also essential because after millions of iterations the state of our
Markov chain would have humongous numerators and denominators if its
components were expressed in infinite precision rational numbers.

\subsection{Sampling}

However, we do have one very difficult issue using ordinary computer
arithmetic in the sampler.  This issue is evaluating
$x_i^{\alpha_i - 1}$ where $x_i$ is some component of either the
current state or the proposal in OC, which happens when we evaluate
$h_3(y)$ or $h_3(w)$ in \eqref{eq:metropolis-ratio} or
\eqref{eq:ratio-beta-hastings} or \eqref{eq:ratio-independence}.
We must assure $x_i > 0$,
and ordinary computer arithmetic cannot guarantee that.
Similarly, when we evaluate
$$
   \left( \frac{w - w_\text{low}}{y - w_\text{low}}
   \right)^{\alpha_\text{low} - 1}
   \left( \frac{w_\text{hig} - w}{w_\text{hig} - y}
   \right)^{\alpha_\text{hig} - 1}
$$
in \eqref{eq:ratio-beta-hastings}, we must assure
\begin{align*}
   w - w_\text{low} & > 0
   \\
   y - w_\text{low} & > 0
   \\
   w_\text{hig} - w & > 0
   \\
   w_\text{hig} - y & > 0
\end{align*}
and ordinary computer arithmetic cannot guarantee that.

In neither case is it sufficient to merely round negative numbers to zero.
We cannot evaluate $x_i^{\alpha_i - 1}$ when $\alpha_i < 1$ and $x_i \le 0$.
We have to guarantee the proposal in OC has all components strictly positive.

What we do in this case is what the Metropolis-Hastings algorithm usually
does with proposals outside of the state space: it rejects them with
probability one.  So all we need to do is be careful and get this case right.

This extra care does not affect the validity of our sampler.  Such validity
is usually talked about pretending that the computer has real real numbers
(with an infinite number of decimal places).  Our algorithm is correct in
that sense.  No one takes inexact computer arithmetic fully into account
when discussing MCMC.

\begin{thebibliography}{}

\bibitem[B\'elisle et~al.(1993)B\'{e}lisle, Romeijn and Smith]{hit-run-one}
B\'{e}lisle, C. J.~P., Romeijn, H.~E., and Smith, R.~L. (1993).
\newblock Hit-and-run algorithms for generating multivariate distributions.
\newblock \emph{Mathematics of Operations Research}, \textbf{18}, 255--266.

\bibitem[Chen and Schmeiser(1993)]{hit-run-two}
Chen, M.-H. and Schmeiser, B. (1993).
\newblock Performance of the Gibbs, hit-and-run, and Metropolis samplers.
\newblock \emph{Journal of Computational and Graphical Statistics},
   \textbf{2}, 251--272.

\bibitem[Geyer(2011)]{geyer-intro}
Geyer, C. J. (2011).
\newblock Introduction to Markov chain Monte Carlo.
\newblock In \emph{Handbook of Markov Chain Monte Carlo}, edited by
    Brooks, S., Gelman, A., Jones, G., and Meng, X.-L., pp.~3--48.
\newblock Boca Raton, FL: Chapman \& Hall/CRC.

\bibitem[Geyer and Johnson(2014)]{mcmc}
Geyer, C.~J. and Johnson, L.~T. (2014).
\newblock R package \texttt{mcmc}: Markov chain Monte Carlo,
    version 0.9-3.
\newblock \url{http://cran.r-project.org/package=mcmc}.

\bibitem[Geyer and Meeden(2013)]{geyer-meeden}
Geyer, C.~J. and Meeden, G.~D. (2013).
\newblock Asymptotics for Constrained Dirichlet Distributions.
\newblock \emph{Bayesian Analysis}, \textbf{8}, 89--110.

\bibitem[Geyer, et al.(2014)Geyer, Meeden, and Fukuda]{rcdd}
Geyer, C.~J., Meeden, G.~D. and Fukuda, K. (2014).
\newblock R package \texttt{rcdd}: Computational geometry,
    version 1.1-8.
\newblock \url{http://cran.r-project.org/package=rcdd}

\bibitem[Hastings(1970)]{hastings}
Hastings, W.~K. (1970).
\newblock Monte Carlo sampling methods using Markov chains and their
    applications.
\newblock \emph{Biometrika}, \textbf{57}, 97--109.

\bibitem[{Lazar et~al.(2008)Lazar, Meeden, and Nelson}]{lazar-meeden-nelson}
Lazar, R., Meeden, G., and Nelson, D. (2008).
\newblock A noninformative {B}ayesian approach to finite population sampling
  using auxiliary variables.
\newblock \emph{Survey Methodology}, \textbf{34}, 51--64.

\bibitem[{Meeden and Lazar(2014)}]{polyapost}
Meeden, G. and Lazar, R. (2014).
\newblock R package \texttt{polyapost}: Simulating from the Polya posterior,
    version 1.1-6.
\newblock \url{http://cran.r-project.org/package=polyapost}.

\bibitem[Metropolis, et al.(1953)Metropolis, Rosenbluth, Rosenbluth, Teller
    and Teller]{metropolis-et-al}
Metropolis, N., Rosenbluth, A.~W., Rosenbluth, M.~N., Teller, A.~H., and
    Teller, E. (1953).
\newblock Equation of state calculations by fast computing machines.
\newblock \emph{Journal of Chemical Physics}, \textbf{21}, 1087--1092.

\bibitem[R Core Team(2014)]{r}
R Core Team (2014).
\newblock R: A language and environment for statistical computing.
\newblock Vienna: R Foundation for Statistical Computing.
\newblock \url{http://www.R-project.org/}.

\bibitem[Smith(1984)]{smith}
Smith, R.~L. (1984).
\newblock "Efficient Monte Carlo procedures for generating points
    uniformly distributed over bounded regions.
\newblock \emph{Operations Research}, \textbf{32}, 1296--1308.

\bibitem[Tierney(1994)]{tierney}
Tierney, L. (1994).
\newblock Markov chains for exploring posterior distributions (with discussion).
\newblock \emph{Annals of Statistics}, \textbf{22}, 1701--1762.

\bibitem[van Valkenhoef(2014)]{hitandrun}
van Valkenhoef, G. (2014).
\newblock R package \texttt{hitandrun}: ``Hit and run'' method for sampling
    uniformly from convex shapes, version 0.4-1.
\newblock \url{http://cran.r-project.org/package=hitandrun}.

\bibitem[Walker(1977)]{walker}
Walker, A.~J. (1977).
\newblock An efficient method for generating discrete random variables
    with general distributions.
\newblock \emph{ACM Transactions on Mathematical Software}, \textbf{3},
    253--256.

\end{thebibliography}

\end{document}

