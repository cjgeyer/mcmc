
\documentclass{article}

\usepackage{natbib}
\usepackage{graphics}
\usepackage{amsmath, amssymb}
\usepackage{indentfirst}
\usepackage[utf8]{inputenc}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\E}{E}

% \VignetteIndexEntry{MCMC Morph Example}

\begin{document}

<<foo,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
foo <- packageDescription("mcmc")
@

\title{Morphemetric MCMC Example (mcmc Version \Sexpr{foo$Version})}
% $ (Just to make emacs syntax highlighting work properly)
\author{Leif T. Johnson \\ and Charles J. Geyer}
\maketitle

\section{Overview}

This is an example how to use morphemetric Markov chains as implemented in
the \verb@mcmc@ package in R.

Let $X$ be a $\mathbb{R}^k$ valued random variable with probability density
function (pdf), $f_X$.  Let $g$ be a diffeomorphism, and $Y=g(X)$.  Then the 
pdf of $Y$, $f_Y$ is given by the equation
\begin{equation}\label{eq:def-fy}
  f_Y(y) = f_X\bigl(g^{-1}(y)\bigr) | \nabla g^{-1}(y) |.
\end{equation}
Since $g$ is a diffeomorphism, we can draw inference about $X$ from information 
about $Y$ (and vice versa).  

It is not unusual for $f_X$ to either be known only up to a normalizing 
constant, or to be analytically intractable in other ways --- such as a large 
number of dimensions.  A common solution to this problem is to use Markov chain
Monte Carlo (MCMC) methods to learn about $f_X$ when this happens.

When using MCMC, a primary concern of the practicioner should be the question 
"Does the Markov chain converge fast enough to be useful?"  One very useful 
convergence rate is called \emph{geometrically ergodic} \textbf{CITE SOMETHING 
HERE}.

The \texttt{mcmc} package implements the Metropolis random-walk algorithm for 
arbitrary log unnormalized probability densities.  But the Metropolis 
random-walk algorithm does not always perform well.  As is demonstrated in 
\citet{johnson-geyer}, for one-to-one densities, $f_X$ and $f_Y$ defined as in
\eqref{eq:def-fy} a Metropolis random-walk $f_Y$ can be geometrically ergodic 
even though a Metropolis random-walk for $f_X$ is not.  Since the densities are
one-to-one, inference about $f_X$ can be drawn from the Markov chain for $f_Y$.

The \texttt{morph.metrop} and \texttt{morph} functions in the \texttt{mcmc} 
package provide this functionality, and this vignette gives a demonstration
on how to use them.

\section{$T$ Distribution}

\citet{johnson-geyer}[Example 4.2] show that a T distribution is
sub-exponentially light, and hence a Metropolis random-walk for a $T$
density cannot be geometrically ergodic, but, using the transformations
described in their Corollaries 1 and 2 will induce a target density,
$\pi_\gamma$ for which a Metropolis random-walk will be geometrically
ergodic.  In fact, we know from \citet{mengersen-tweedie} that for a
univariate target density, being exponentially light is a sufficient
condition for the geometric ergodicity of a Metropolis random-walk.  Hence
using the transformation described as $h_2$ in
\citet{johnson-geyer}[Corollary 2] will induce a target density for which a
Metroplis random-walk will be geometrically ergodic.  This section
demonstrates how to do this using the \texttt{mcmc} package for
\texttt{R}.  We will use a univariate $T$ distribution with 3 degrees of
freedom for a target density.

Passing a positive value for \texttt{b} to \texttt{morph} function will
create the aforementioned transformation, $h_2$.  It's as simple as 
<<>>=
library(mcmc)
h2 <- morph(b=1)
@ 
We can now see the induced density.  Note that \texttt{morph} works for
log unnormalized densities, so we need exponentiate the induced density to
plot it on the usual scale.
<<>>=
lud <- function(x) dt(x, df=3, log=TRUE)
lud.induced <- h2$lud(lud)
@ 
We can plot the two densities,
<<fig=TRUE>>=
x <- seq(-3, 3, length.out=400)
plot(x, exp(sapply(x, lud.induced)), lty=2, type="l",
     xlab="t", ylab="")
lines(x, exp(lud(x)), lty=1)
legend("topright", c("t-density", "induced density"), lty=1:2)
@ 
The \texttt{sapply} in this example is necessary, \texttt{lud.induced} is not 
vectorized.  Instead, it treats any vector passed as a single input, which
is rescaled (using an isomorphic transformation) and passed to
\texttt{lud}.  Compare the behavior of \texttt{lud} and
\texttt{lud.induced} in the following example.
<<>>=
lud(rep(0, 4))
lud(1)
lud.induced(rep(0, 4))
lud.induced(1)
@
Because \texttt{dt} is vectorized, \texttt{lud} is vectorized so it maps 
vectors to vectors, but \texttt{lud.induced} treats the vector as a single
input, which is transformed and passed to \texttt{lud}.

Running a Markov chain for the induced density is done with
\texttt{morph.metrop}.
<<>>=
out <- morph.metrop(lud, 0, blen=100, nbatch=100, morph=morph(b=1))
@ 
The content of \texttt{out\$batch} is on the scale of used by
\texttt{lud}.  Once the transformation has been set, no adjustment is
needed (unless you want to change transformations).  We start by adjusting
the scale.
<<>>=
# adjust scale to find a roughly 20% acceptance rate
out$accept
@ 
An acceptance rate of \Sexpr{round(100 * out$accept, 1)}\%
%$ to fix emacs highlighting
is probably too high.  By increasing the scale of the proposal distribution
we can bring it down towards 20\%.
<<>>=
out <- morph.metrop(out, scale=4)
out$accept
@ 
We now use this Markov chain to estimate 
\begin{equation}\label{eq:e-abs-t}
  \E(|T|)
\end{equation}
for the $T$ distribution with 3 degrees of freedom.  We'll start with an
estimate from 1000 iterations of the chain.  
<<>>=
out <- morph.metrop(out, blen=1, nbatch=1e4, outfun=abs)
@ 
We're going to use over-lapping batch means to estimate \eqref{eq:e-abs-t},
but first we need to establish a reasonable batch length.
<<fig=TRUE>>=
acf(out$batch, lag.max=200)
@ 
It looks like a batch length of 100 will be sufficiently long.
<<>>=
mcse <- sqrt(olbm(out$batch, 100))
est.mean <- colMeans(out$batch)
est.mean
mcse
@ 
The variable \texttt{est.mean} gives our MCMC estimate of
\eqref{eq:e-abs-t} and \texttt{mcse} gives the Monte Carlo standard error
of our estimate.  Because the underlying Markov chain is geometrically
ergodic and $\E(|T|^2)$ is finite, there is a central limit theorem for our
estimate.  The confidence interval for \eqref{eq:e-abs-t} is given by
<<>>=
est.mean + mcse * 1.96 * c(-1,1)
@ 
If a narrower confidence interval is desired, the Markov chain can be run
longer.

Note that when calculating our estimate and the Monte Carlo standard error
we were not concerned with what was happening on the transformed scale.

\section{Multinomial Distribution with a Conjugate Prior}

\begin{thebibliography}{}

\bibitem[Gelman et al.(1996)Gelman, Roberts, and Gilks]{grg}
Gelman, A., G.~O. Roberts, and W.~R. Gilks (1996).
\newblock Efficient Metropolis jumping rules.
\newblock In \emph{Bayesian Statistics, 5 (Alicante, 1994)}, pp.~599--607.
  Oxford University Press.

\bibitem[Geyer(1992)]{practical}
Geyer, C.~J. (1992).
\newblock Practical Markov chain Monte Carlo (with discussion).
\newblock \emph{Statistical Science}, 7, 473--511.

\bibitem[Geyer and Thompson(1995)]{geyer-temp}
Geyer, C.~J. and E.~A. Thompson (1995).
\newblock Annealing Markov chain Monte Carlo with applications to
    ancestral inference.
\newblock \emph{Journal of the American Statistical Association}, 90, 909--920.

\bibitem[Johnson and Geyer(submitted)]{johnson-geyer}
Johnson, L.~T. and C.~J. Geyer (submitted).
\newblock Variable Transformation to Obtain Geometric Ergodicity
    in the Random-walk Metropolis Algorithm.
\newblock \emph{Annals of Statistics}.

\bibitem[Mengersen and Tweedie(1996)]{mengersen-tweedie}
  Mengersen, K.L. ad R. L. Tweedie (1996).
\newblock Rates of convergence of the {H}astings and {M}etropolis algorithms.
\newblock \emph{Annals of Statistics}, 24, 101--121.

\end{thebibliography}

\end{document}
